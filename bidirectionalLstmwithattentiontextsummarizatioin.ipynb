{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwtpOw8LFyUI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "a81e1a53-8cab-4842-eb5b-3ad3f8909a4d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_Y7B2gCF6tz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import winsound as ws\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "\n",
        "\n",
        "CNN_data=\"C:\\\\text mining\\\\data\\\\new CNN\\\\cnn\\\\\"\n",
        "daily_data=\"C:\\\\text mining\\\\data\\\\new CNN\\\\dailymail\\\\\"\n",
        "\n",
        "datasets={\"cnn\":CNN_data,\"dailymail\":daily_data}\n",
        "\n",
        "data_categories=[\"training\",\"validation\",\"test\"]\n",
        "\n",
        "data={\"articles\":[],\"summaries\":[]}\n",
        "\n",
        "\n",
        "\n",
        "def parsetext(dire,category,filename):\n",
        "    with open(\"%s\\\\%s\"%(dire+category,filename),'r',encoding=\"Latin-1\") as readin:\n",
        "        print(\"file read successfully\")\n",
        "        text=readin.read()\n",
        "    return text.lower()\n",
        "\n",
        "\n",
        "def load_data(dire,category):\n",
        "    \"\"\"dataname refers to either training, test or validation\"\"\"\n",
        "    for dirs,subdr, files in os.walk(dire+category):\n",
        "        filenames=files\n",
        "    return filenames\n",
        "\n",
        "\n",
        "def cleantext(text):\n",
        "    text=re.sub(r\"what's\",\"what is \",text)\n",
        "    text=re.sub(r\"it's\",\"it is \",text)\n",
        "    text=re.sub(r\"\\'ve\",\" have \",text)\n",
        "    text=re.sub(r\"i'm\",\"i am \",text)\n",
        "    text=re.sub(r\"\\'re\",\" are \",text)\n",
        "    text=re.sub(r\"n't\",\" not \",text)\n",
        "    text=re.sub(r\"\\'d\",\" would \",text)\n",
        "    text=re.sub(r\"\\'s\",\"s\",text)\n",
        "    text=re.sub(r\"\\'ll\",\" will \",text)\n",
        "    text=re.sub(r\"can't\",\" cannot \",text)\n",
        "    text=re.sub(r\" e g \",\" eg \",text)\n",
        "    text=re.sub(r\"e-mail\",\"email\",text)\n",
        "    text=re.sub(r\"9\\\\/11\",\" 911 \",text)\n",
        "    text=re.sub(r\" u.s\",\" american \",text)\n",
        "    text=re.sub(r\" u.n\",\" united nations \",text)\n",
        "    text=re.sub(r\"\\n\",\" \",text)\n",
        "    text=re.sub(r\":\",\" \",text)\n",
        "    text=re.sub(r\"-\",\" \",text)\n",
        "    text=re.sub(r\"\\_\",\" \",text)\n",
        "    text=re.sub(r\"\\d+\",\" \",text)\n",
        "    text=re.sub(r\"[$#@%&*!~?%{}()]\",\" \",text)\n",
        "    \n",
        "    return text\n",
        "\n",
        "def printArticlesum(k):\n",
        "    print(\"---------------------original sentence-----------------------\")\n",
        "    print(\"-------------------------------------------------------------\")\n",
        "    print(data[\"articles\"][k])\n",
        "    print(\"----------------------Summary sentence-----------------------\")\n",
        "    print(\"-------------------------------------------------------------\")\n",
        "    print(data[\"summaries\"][k])\n",
        "    return 0\n",
        "\n",
        "\n",
        "def announcedone():\n",
        "    duration=2000\n",
        "    freq=440\n",
        "    ws.Beep(freq,duration)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "filenames=load_data(datasets[\"cnn\"],data_categories[0])\n",
        "\n",
        "\"\"\"----------load the data, sentences and summaries-----------\"\"\"\n",
        "for k in range(len(filenames[:400])):\n",
        "        if k%2==0:\n",
        "            try:\n",
        "                data[\"articles\"].append(cleantext(parsetext(datasets[\"cnn\"],data_categories[0],\"%s\"%filenames[k])))\n",
        "            except Exception as e:\n",
        "                data[\"articles\"].append(\"Could not read\")\n",
        "                print(e)\n",
        "        else:\n",
        "            try:\n",
        "                data[\"summaries\"].append(cleantext(parsetext(datasets[\"cnn\"],data_categories[0],\"%s\"%filenames[k])))\n",
        "            except Exception as e:\n",
        "                data[\"summaries\"].append(\"Could not read\")\n",
        "                print(e)\n",
        "\n",
        "del filenames"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKSm4y3kHFI1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# word to vec\n",
        "\n",
        "from numpy.random import seed\n",
        "seed(1)\n",
        "\n",
        "modelLocation=\"C:/text mining/TrainedModels/\"\n",
        "\n",
        "\n",
        "import gensim as gs\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy as sc\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize as wt\n",
        "from nltk.tokenize import sent_tokenize as st\n",
        "from numpy import argmax\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import logging\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
        "    level=logging.INFO)\n",
        "\n",
        "emb_size_all = 300\n",
        "maxcorp=5000\n",
        "\n",
        "\n",
        "def createCorpus(t):\n",
        "    corpus = []\n",
        "    all_sent = []\n",
        "    for k in t:\n",
        "        for p in t[k]:\n",
        "            corpus.append(st(p))\n",
        "    for sent in range(len(corpus)):\n",
        "        for k in corpus[sent]:\n",
        "            all_sent.append(k)\n",
        "    for m in range(len(all_sent)):\n",
        "        all_sent[m] = wt(all_sent[m])\n",
        "    \n",
        "    all_words=[]\n",
        "    for sent in all_sent:\n",
        "        hold=[]\n",
        "        for word in sent:\n",
        "            hold.append(word.lower())\n",
        "        all_words.append(hold)\n",
        "    return all_words\n",
        "\n",
        "\n",
        "def word2vecmodel(corpus):\n",
        "    emb_size = emb_size_all\n",
        "    model_type={\"skip_gram\":1,\"CBOW\":0}\n",
        "    window=10\n",
        "    workers=4\n",
        "    min_count=4\n",
        "    batch_words=20\n",
        "    epochs=25\n",
        "    #include bigrams\n",
        "    #bigramer = gs.models.Phrases(corpus)\n",
        "\n",
        "    model=gs.models.Word2Vec(corpus,size=emb_size,sg=model_type[\"skip_gram\"],\n",
        "                             compute_loss=True,window=window,min_count=min_count,workers=workers,\n",
        "                             batch_words=batch_words)\n",
        "        \n",
        "    model.train(corpus,total_examples=len(corpus),epochs=epochs)\n",
        "    model.save(\"%sWord2vec\"%modelLocation)\n",
        "    print('\\007')\n",
        "    return model\n",
        "\n",
        "\n",
        "def summonehot(corpus):\n",
        "    allwords=[]\n",
        "    annotated={}\n",
        "    for sent in corpus:\n",
        "        for word in wt(sent):\n",
        "            allwords.append(word.lower())\n",
        "    print(len(set(allwords)), \"unique characters in corpus\")\n",
        "    #maxcorp=int(input(\"Enter desired number of vocabulary: \"))\n",
        "    maxcorp=int(len(set(allwords))/1.1)\n",
        "    wordcount = Counter(allwords).most_common(maxcorp)\n",
        "    allwords=[]\n",
        "    \n",
        "    for p in wordcount:\n",
        "        allwords.append(p[0])  \n",
        "        \n",
        "    allwords=list(set(allwords))\n",
        "    \n",
        "    print(len(allwords), \"unique characters in corpus after max corpus cut\")\n",
        "    #integer encode\n",
        "    label_encoder = LabelEncoder()\n",
        "    integer_encoded = label_encoder.fit_transform(allwords)\n",
        "    #one hot\n",
        "    onehot_encoder = OneHotEncoder(sparse=False)\n",
        "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
        "    #make look up dict\n",
        "    for k in range(len(onehot_encoded)): \n",
        "        inverted = cleantext(label_encoder.inverse_transform([argmax(onehot_encoded[k, :])])[0]).strip()\n",
        "        annotated[inverted]=onehot_encoded[k]\n",
        "    return label_encoder,onehot_encoded,annotated\n",
        "\n",
        "\n",
        "\n",
        "def wordvecmatrix(model,data):\n",
        "    IO_data={\"article\":[],\"summaries\":[]}\n",
        "    i=1\n",
        "    for k in range(len(data[\"articles\"])):\n",
        "        art=[]\n",
        "        summ=[]\n",
        "        for word in wt(data[\"articles\"][k].lower()):\n",
        "            try:\n",
        "                art.append(model.wv.word_vec(word))\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "\n",
        "        for word in wt(data[\"summaries\"][k].lower()):\n",
        "            try:\n",
        "                summ.append(onehot[word])\n",
        "                #summ.append(model.wv.word_vec(word))\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "        \n",
        "        IO_data[\"article\"].append(art) \n",
        "        IO_data[\"summaries\"].append(summ)\n",
        "        if i%100==0:\n",
        "            print(\"progress: \" + str(((i*100)/len(data[\"articles\"]))))\n",
        "        i+=1\n",
        "    #announcedone()\n",
        "    print('\\007')\n",
        "    return IO_data\n",
        "\n",
        "def cutoffSequences(data,artLen,sumlen):\n",
        "    data2={\"article\":[],\"summaries\":[]}\n",
        "    for k in range(len(data[\"article\"])):\n",
        "        if len(data[\"article\"][k])<artLen or len(data[\"summaries\"][k])<sumlen:\n",
        "             #data[\"article\"]=np.delete(data[\"article\"],k,0)\n",
        "             #data[\"article\"]=np.delete(data[\"summaries\"],k,0)\n",
        "             pass\n",
        "        else:\n",
        "            data2[\"article\"].append(data[\"article\"][k][:artLen])\n",
        "            data2[\"summaries\"].append(data[\"summaries\"][k][:sumlen])\n",
        "    return data2\n",
        "\n",
        "\n",
        "def max_len(data):\n",
        "    lenk=[]\n",
        "    for k in data:\n",
        "        lenk.append(len(k))\n",
        "    print(\"The minimum length is: \",min(lenk))\n",
        "    print(\"The average length is: \",np.average(lenk))\n",
        "    print(\"The max length is: \",max(lenk))\n",
        "    return min(lenk),max(lenk)\n",
        "\n",
        "\"\"\"reshape vectres for Gensim\"\"\"\n",
        "def reshape(vec):\n",
        "    return np.reshape(vec,(1,emb_size_all))\n",
        "\n",
        "def addones(seq):\n",
        "    return np.insert(seq, [0], [[0],], axis = 0)\n",
        "\n",
        "def endseq(seq):\n",
        "    pp=len(seq)\n",
        "    return np.insert(seq, [pp], [[1],], axis = 0)\n",
        "\n",
        "\n",
        "corpus = createCorpus(data)\n",
        "\n",
        "label_encoder,onehot_encoded,onehot=summonehot(data[\"summaries\"])\n",
        "\n",
        "model=word2vecmodel(corpus)\n",
        "\n",
        "model.get_latest_training_loss()\n",
        "\n",
        "train_data = wordvecmatrix(model,data)\n",
        "\n",
        "print(len(train_data[\"article\"]), \"training articles\")\n",
        "\n",
        "train_data=cutoffSequences(train_data,300,25)\n",
        "\n",
        "#seq length stats\n",
        "max_len(train_data[\"article\"])\n",
        "max_len(train_data[\"summaries\"])\n",
        "\n",
        "\n",
        "train_data[\"summaries\"]=np.array(train_data[\"summaries\"])\n",
        "train_data[\"article\"]=np.array(train_data[\"article\"])\n",
        "\n",
        "\n",
        "#add end sequence for each article\n",
        "\n",
        "#train_data[\"summaries\"]=np.array(list(map(endseq,train_data[\"summaries\"])))\n",
        "#train_data[\"article\"]=np.array(list(map(endseq,train_data[\"article\"])))\n",
        "\n",
        "print(\"summary length: \",len(train_data[\"summaries\"][0]))\n",
        "print(\"article length: \",len(train_data[\"article\"][0]))\n",
        "\n",
        "\n",
        "\"\"\"__pad sequences__\n",
        "train_data[\"article\"]=pad_sequences(train_data[\"article\"],maxlen=max_len(train_data[\"article\"]),\n",
        "          padding='post',dtype=float)\n",
        "train_data[\"summaries\"]=pad_sequences(train_data[\"summaries\"],maxlen=max_len(train_data[\"summaries\"]),\n",
        "          padding='post',dtype=float)\n",
        "\"\"\"\n",
        "#add start sequence\n",
        "train_data[\"summaries\"]=np.array(list(map(addones,train_data[\"summaries\"])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lp98LuNCKVdY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(1)\n",
        "\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "import logging\n",
        "\n",
        "import plotly.plotly as py\n",
        "import plotly.graph_objs as go\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import pydot\n",
        "\n",
        "\n",
        "import keras\n",
        "from keras import backend as k\n",
        "k.set_learning_phase(1)\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras import initializers\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.models import Sequential,Model\n",
        "from keras.layers import Dense,LSTM,Dropout,Input,Activation,Add,Concatenate\n",
        "from keras.layers.advanced_activations import LeakyReLU,PReLU\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "from keras.optimizers import Adam\n",
        "#keras.utils.vis_utils import plot_model\n",
        "\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
        "    level=logging.INFO)\n",
        "\n",
        "\n",
        "batch_size = 100\n",
        "num_classes = 1\n",
        "epochs = 5\n",
        "hidden_units = emb_size_all\n",
        "learning_rate = 0.002\n",
        "clip_norm = 1.0\n",
        "\n",
        "\n",
        "en_shape=np.shape(train_data[\"article\"][0])\n",
        "de_shape=np.shape(train_data[\"summaries\"][0])\n",
        "\n",
        "\"\"\"______generate summary length for test______\"\"\"\n",
        "#train_data[\"nums_summ\"]=list(map(lambda x:0 if len(x)<5000 else 1,data[\"articles\"]))\n",
        "#train_data[\"nums_summ\"]=list(map(len,data[\"summaries\"]))\n",
        "#train_data[\"nums_summ_norm\"]=(np.array(train_data[\"nums_summ\"])-min(train_data[\"nums_summ\"]))/(max(train_data[\"nums_summ\"])-min(train_data[\"nums_summ\"]))\n",
        "\n",
        "\n",
        "\n",
        "def encoder_decoder(data):\n",
        "    print('Encoder_Decoder LSTM...')\n",
        "   \n",
        "    \"\"\"__encoder___\"\"\"\n",
        "    encoder_inputs = Input(shape=en_shape)\n",
        "    \n",
        "    encoder_LSTM = LSTM(hidden_units, dropout_U = 0.2, dropout_W = 0.2 ,return_state=True)\n",
        "    encoder_LSTM_rev=LSTM(hidden_units,return_state=True,go_backwards=True)\n",
        "    \n",
        "    #merger=Add()[encoder_LSTM(encoder_inputs), encoder_LSTM_rev(encoder_inputs)]\n",
        "    encoder_outputsR, state_hR, state_cR = encoder_LSTM_rev(encoder_inputs)\n",
        "    encoder_outputs, state_h, state_c = encoder_LSTM(encoder_inputs)\n",
        "    \n",
        "    state_hfinal=Add()([state_h,state_hR])\n",
        "    state_cfinal=Add()([state_c,state_cR])\n",
        "    \n",
        "    encoder_states = [state_hfinal,state_cfinal]\n",
        "    \n",
        "    \n",
        "    decoder_inputs = Input(shape=(None,de_shape[1]))\n",
        "    decoder_LSTM = LSTM(hidden_units,return_sequences=True,return_state=True)\n",
        "    decoder_outputs, _, _ = decoder_LSTM(decoder_inputs,initial_state=encoder_states) \n",
        "    decoder_dense = Dense(de_shape[1],activation='linear')\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    \n",
        "    model= Model(inputs=[encoder_inputs,decoder_inputs], outputs=decoder_outputs)\n",
        "    #plot_model(model, to_file=modelLocation+'model.png', show_shapes=True)\n",
        "    rmsprop = RMSprop(lr=learning_rate,clipnorm=clip_norm)\n",
        "    \n",
        "    model.compile(loss='mse',optimizer=rmsprop)\n",
        "\n",
        "    x_train,x_test,y_train,y_test=tts(data[\"article\"],data[\"summaries\"],test_size=0.20)\n",
        "    model.fit(x=[x_train,y_train],\n",
        "              y=y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              verbose=1,\n",
        "              validation_data=([x_test,y_test], y_test))\n",
        "    \n",
        "    \n",
        "    encoder_model_inf = Model(encoder_inputs,encoder_states)\n",
        "    \n",
        "    decoder_state_input_H = Input(shape=(hidden_units,))\n",
        "    decoder_state_input_C = Input(shape=(hidden_units,)) \n",
        "    decoder_state_inputs = [decoder_state_input_H, decoder_state_input_C]\n",
        "    decoder_outputs, decoder_state_h, decoder_state_c = decoder_LSTM(decoder_inputs,\n",
        "                                                                     initial_state=decoder_state_inputs)\n",
        "    decoder_states = [decoder_state_h, decoder_state_c]\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    \n",
        "    decoder_model_inf= Model([decoder_inputs]+decoder_state_inputs,\n",
        "                         [decoder_outputs]+decoder_states)\n",
        "    \n",
        "    \n",
        "    #plot_model(encoder_model_inf, to_file='encoder_model.png', show_shapes=True)\n",
        "    #plot_model(decoder_model_inf, to_file='decoder_model.png', show_shapes=True)\n",
        "    scores = model.evaluate([x_test,y_test],y_test, verbose=0)\n",
        "    \n",
        "    \n",
        "    print('LSTM test scores:', scores)\n",
        "    #announcedone()\n",
        "    print('\\007')\n",
        "    print(model.summary())\n",
        "    return model,encoder_model_inf,decoder_model_inf\n",
        "\n",
        "\n",
        "def comparePred(index):\n",
        "    pred=trained_model.predict([np.reshape(train_data[\"article\"][index],(1,en_shape[0],emb_size_all)),np.reshape(train_data[\"summaries\"][index],(1,de_shape[0],emb_size_all))])\n",
        "    return pred\n",
        "\n",
        "def generateText(SentOfVecs):\n",
        "    SentOfVecs=np.reshape(SentOfVecs,de_shape)\n",
        "    kk=\"\"\n",
        "    for k in SentOfVecs:\n",
        "        kk=kk+((getWord(k)[0]+\" \") if getWord(k)[1]>0.2 else \"\")\n",
        "    return kk\n",
        "\n",
        "\n",
        "\n",
        "def summarize(article):\n",
        "    stop_pred = False\n",
        "    article =  np.reshape(article,(1,en_shape[0],en_shape[1]))\n",
        "    \n",
        "    #get initial h and c values from encoder\n",
        "    init_state_val = encoder.predict(article)\n",
        "    target_seq = np.zeros((1,1,emb_size_all))\n",
        "    \n",
        "    generated_summary=[]\n",
        "    while not stop_pred:\n",
        "        decoder_out,decoder_h,decoder_c= decoder.predict(x=[target_seq]+init_state_val)\n",
        "        generated_summary.append(decoder_out)\n",
        "        init_state_val= [decoder_h,decoder_c]\n",
        "        #get most similar word and put in line to be input in next timestep\n",
        "        #target_seq=np.reshape(model.wv[getWord(decoder_out)[0]],(1,1,emb_size_all))\n",
        "        target_seq=np.reshape(decoder_out,(1,1,emb_size_all))\n",
        "        if len(generated_summary)== de_shape[0]:\n",
        "            stop_pred=True\n",
        "            break\n",
        "    return generated_summary\n",
        "        \n",
        "        \n",
        "\n",
        "\n",
        "trained_model,encoder,decoder = encoder_decoder(train_data)\n",
        "\n",
        "def saveModels():\n",
        "    trained_model.save(\"%sinit_model\"%modelLocation)\n",
        "    encoder.save(\"%sencoder\"%modelLocation)\n",
        "    decoder.save(\"%sdecoder\"%modelLocation)\n",
        "\n",
        "print(generateText(summarize(train_data[\"article\"][10])))\n",
        "print(data[\"summaries\"][10])\n",
        "print(data[\"articles\"][10])\n",
        "\n",
        "del trained_model,encoder,decoder\n",
        "\n",
        "\n",
        "getWord(collect_pred[23])\n",
        "model.wv.most_similar(np.zeros((1,emb_size_all)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpD7Ov8wHnkg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "6dc71ff0-362a-4a7a-8d51-ba5e7d5b9c73"
      },
      "source": [
        "#lstm attention\n",
        "from numpy.random import seed\n",
        "seed(1)\n",
        "\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "import logging\n",
        "\n",
        "from pyrouge import Rouge155 \n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "from keras import backend as k\n",
        "k.set_learning_phase(1)\n",
        "from keras import initializers\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense,LSTM,Input,Activation,Add,TimeDistributed,\\\n",
        "Permute,Flatten,RepeatVector,merge,Lambda,Multiply,Reshape\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
        "    level=logging.INFO)\n",
        "\n",
        "\n",
        "batch_size = 50\n",
        "num_classes = 1\n",
        "epochs = 20\n",
        "hidden_units = emb_size_all\n",
        "learning_rate = 0.005\n",
        "clip_norm = 2.0\n",
        "\n",
        "en_shape=np.shape(train_data[\"article\"][0])\n",
        "de_shape=np.shape(train_data[\"summaries\"][0])\n",
        "\n",
        "   \n",
        "\n",
        "def encoder_decoder(data):\n",
        "    print('Encoder_Decoder LSTM...')\n",
        "   \n",
        "    \"\"\"__encoder___\"\"\"\n",
        "    encoder_inputs = Input(shape=en_shape)\n",
        "    \n",
        "    encoder_LSTM = LSTM(hidden_units,dropout_U=0.2,dropout_W=0.2,return_sequences=True,return_state=True)\n",
        "    encoder_LSTM_rev=LSTM(hidden_units,return_state=True,return_sequences=True,dropout_U=0.05,dropout_W=0.05,go_backwards=True)\n",
        "    \n",
        "    encoder_outputs, state_h, state_c = encoder_LSTM(encoder_inputs)\n",
        "    encoder_outputsR, state_hR, state_cR = encoder_LSTM_rev(encoder_inputs)\n",
        "    \n",
        "    state_hfinal=Add()([state_h,state_hR])\n",
        "    state_cfinal=Add()([state_c,state_cR])\n",
        "    encoder_outputs_final = Add()([encoder_outputs,encoder_outputsR])\n",
        "    \n",
        "    encoder_states = [state_hfinal,state_cfinal]\n",
        "    \n",
        "    \"\"\"____decoder___\"\"\"\n",
        "    decoder_inputs = Input(shape=(None,de_shape[1]))\n",
        "    decoder_LSTM = LSTM(hidden_units,return_sequences=True,dropout_U=0.2,dropout_W=0.2,return_state=True)\n",
        "    decoder_outputs, _, _ = decoder_LSTM(decoder_inputs,initial_state=encoder_states)\n",
        "    \n",
        "    #Pull out XGBoost, (I mean attention)\n",
        "    attention = TimeDistributed(Dense(1, activation = 'tanh'))(encoder_outputs_final)\n",
        "    attention = Flatten()(attention)\n",
        "    attention = Multiply()([decoder_outputs, attention])\n",
        "    attention = Activation('softmax')(attention)\n",
        "    attention = Permute([2, 1])(attention)\n",
        " \n",
        "    decoder_dense = Dense(de_shape[1],activation='softmax')\n",
        "    decoder_outputs = decoder_dense(attention)\n",
        "    \n",
        "    model= Model(inputs=[encoder_inputs,decoder_inputs], outputs=decoder_outputs)\n",
        "    print(model.summary())\n",
        "    \n",
        "    rmsprop = RMSprop(lr=learning_rate,clipnorm=clip_norm)\n",
        "    model.compile(loss='categorical_crossentropy',optimizer=rmsprop,metrics=['accuracy'])\n",
        "    \n",
        "    x_train,x_test,y_train,y_test=tts(data[\"article\"],data[\"summaries\"],test_size=0.20)\n",
        "    history= model.fit(x=[x_train,y_train],\n",
        "              y=y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              verbose=1,\n",
        "              validation_data=([x_test,y_test], y_test))\n",
        "    print(model.summary())\n",
        "    \"\"\"_________________inference mode__________________\"\"\"\n",
        "    encoder_model_inf = Model(encoder_inputs,encoder_states)\n",
        "    \n",
        "    decoder_state_input_H = Input(shape=(en_shape[0],))\n",
        "    decoder_state_input_C = Input(shape=(en_shape[0],)) \n",
        "    decoder_state_inputs = [decoder_state_input_H, decoder_state_input_C]\n",
        "    decoder_outputs, decoder_state_h, decoder_state_c = decoder_LSTM(decoder_inputs,\n",
        "                                                                     initial_state=decoder_state_inputs)\n",
        "    decoder_states = [decoder_state_h, decoder_state_c]\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    \n",
        "    decoder_model_inf= Model([decoder_inputs]+decoder_state_inputs,\n",
        "                         [decoder_outputs]+decoder_states)\n",
        "    \n",
        "    scores = model.evaluate([x_test,y_test],y_test, verbose=1)\n",
        "    \n",
        "    \n",
        "    print('LSTM test scores:', scores)\n",
        "    print('\\007')\n",
        "    print(model.summary())\n",
        "    return model,encoder_model_inf,decoder_model_inf,history\n",
        "\n",
        "\n",
        "\"\"\"_________generate summary from vectors_____________\"\"\"\n",
        "\n",
        "\n",
        "def generateText(SentOfVecs):\n",
        "    SentOfVecs=np.reshape(SentOfVecs,de_shape)\n",
        "    kk=\"\"\n",
        "    for k in SentOfVecs:\n",
        "        kk = kk + label_encoder.inverse_transform([argmax(k)])[0].strip()+\" \"\n",
        "        #kk=kk+((getWord(k)[0]+\" \") if getWord(k)[1]>0.01 else \"\")\n",
        "    return kk\n",
        "\n",
        "\"\"\"________________generate summary vectors___________\"\"\"\n",
        "\n",
        "def summarize(article):\n",
        "    stop_pred = False\n",
        "    article =  np.reshape(article,(1,en_shape[0],en_shape[1]))\n",
        "    #get initial h and c values from encoder\n",
        "    init_state_val = encoder.predict(article)\n",
        "    target_seq = np.zeros((1,1,de_shape[1]))\n",
        "    #target_seq =np.reshape(train_data['summaries'][k][0],(1,1,de_shape[1]))\n",
        "    generated_summary=[]\n",
        "    while not stop_pred:\n",
        "        decoder_out,decoder_h,decoder_c= decoder.predict(x=[target_seq]+init_state_val)\n",
        "        generated_summary.append(decoder_out)\n",
        "        init_state_val= [decoder_h,decoder_c]\n",
        "        #get most similar word and put in line to be input in next timestep\n",
        "        #target_seq=np.reshape(model.wv[getWord(decoder_out)[0]],(1,1,emb_size_all))\n",
        "        target_seq=np.reshape(decoder_out,(1,1,de_shape[1]))\n",
        "        if len(generated_summary)== de_shape[0]:\n",
        "            stop_pred=True\n",
        "            break\n",
        "    return generated_summary\n",
        "\n",
        "\"\"\"__________________Plot training curves_______________\"\"\"\n",
        "\n",
        "def plot_training(history):\n",
        "    print(history.history.keys())\n",
        "    #  \"Accuracy\"\n",
        "    plt.plot(history.history['acc'])\n",
        "    plt.plot(history.history['val_acc'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'validation'], loc='upper left')\n",
        "    plt.show()\n",
        "    # \"Loss\"\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'validation'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def saveModels():\n",
        "    trained_model.save(\"%sinit_model\"%modelLocation)\n",
        "    encoder.save(\"%sencoder\"%modelLocation)\n",
        "    decoder.save(\"%sdecoder\"%modelLocation)\n",
        "    \n",
        "def evaluate_summ(article):\n",
        "    ref=''\n",
        "    for k in wt(data['summaries'][article])[:20]:\n",
        "        ref=ref+' '+k\n",
        "    gen_sum = generateText(summarize(train_data[\"article\"][article]))\n",
        "    print(\"-----------------------------------------------------\")\n",
        "    print(\"Original summary\")\n",
        "    print(ref)\n",
        "    print(\"-----------------------------------------------------\")\n",
        "    print(\"Generated summary\")\n",
        "    print(gen_sum)\n",
        "    print(\"-----------------------------------------------------\")\n",
        "    rouge = Rouge155()\n",
        "    score = rouge.score_summary(ref, gen_sum)\n",
        "    print(\"Rouge1 Score: \",score)\n",
        "        \n",
        "\n",
        "\n",
        "trained_model,encoder,decoder,history = encoder_decoder(train_data)\n",
        "plot_training(history)\n",
        "evaluate_summ(10)\n",
        "\n",
        "print(generateText(summarize(train_data[\"article\"][8])))\n",
        "print(data[\"summaries\"][8])\n",
        "print(data[\"articles\"][78])\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b6a4f5559cbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyrouge\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRouge155\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyrouge'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sK24LRFNKC1x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}